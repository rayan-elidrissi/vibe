<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="VIBE: An open-source web platform to benchmark vision models for gesture-based interaction with immediate feedback on accuracy, latency, and resource usage.">
  <meta name="keywords" content="VIBE, Benchmark, Gesture Interaction, Vision Models, HCI, Pose Estimation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VIBE: Benchmarking Vision Models for Gesture-Based Interaction</title>


  <meta property="og:title" content="VIBE: Benchmarking Vision Models for Gesture-Based Interaction" />
  <meta property="og:description"
    content="VIBE is a web platform for rapid, reproducible benchmarking of vision models for gesture-based interaction with immediate metrics on accuracy, latency, and resource usage." />
  <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
      if you update and want to force Twitter to re-scrape. -->
  <meta property="twitter:card" content="summary" />
  <meta property="twitter:title" content="VIBE: Benchmarking Vision Models for Gesture-Based Interaction" />
  <meta property="twitter:description"
    content="Open-source web platform to benchmark gesture-interaction vision models with immediate feedback on key metrics." />
  <!-- <meta property="twitter:image"         content="https://3dmagicpony.github.io/resources/overview.jpg" /> -->

  <!-- MathJax library -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-TSQGH8Q0WV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-TSQGH8Q0WV');
  </script>
  


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">


  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-4 publication-title" style="font-size: 2rem;">VIBE: An Open-Source Platform for Benchmarking Vision Models for Gesture-Based Interaction</h1>
            <h2 class="subtitle is-6" style="margin-top: 0.5rem;">A research demo for rapid, reproducible, and comparable evaluations</h2>
            <div class="is-size-5 publication-authors" style="margin-top: 0.75rem;">
              <span class="author-block">Rayan El Idrissi Dafali</span>
              &nbsp;&nbsp;&nbsp;
              <span class="author-block">Alina Glushkova</span>
            </div>
            <div class="is-size-6 publication-authors" style="margin-top: 0.25rem; margin-bottom: 10px;">
              <span class="author-block">Mines Paris – PSL, Centre de Robotique, France</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper (PDF)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/rayan-elidrissi/vibe" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://rayan-elidrissi.github.io/vibe/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-video"></i>
                    </span>
                    <span>Demo</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <style>
    #teaser-video {
      max-width: 85%;
      margin: 0 auto;
      display: block;
      border: none;
      border-radius: 4px;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.15);
    }
  </style>
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser-video" autoplay muted loop playsinline height="100%">
          <source src="./resources/demo_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle" style="text-align: center;">Figure 1 – VIBE interface: upload a short video and get immediate model metrics.</h2>
      </div>
    </div>
  </section>

  <section class="section" style="padding-top: 1rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4" style="font-weight: 700;" id="abstract">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Despite rapid progress in vision models, gesture-based interaction remains hard to prototype in Human–Computer Interaction (HCI). We present <b>VIBE</b>, a web platform for benchmarking multiple vision models from short video clips with immediate feedback on accuracy, latency, and resource usage. VIBE lowers evaluation friction and enables fast, reproducible, and comparable model selection for interactive systems.
            </p>
          </div>

          <div class="columns">
            <div class="column">
              <h3 class="title is-5" style="font-weight: 700;">CCS Concepts</h3>
              <div class="content">
                <ul>
                  <li>Human-centered computing → Gestural input</li>
                  <li>Computing methodologies → Computer vision</li>
                  <li>Applied computing → Interactive learning environments</li>
                </ul>
              </div>
            </div>
            <div class="column">
              <h3 class="title is-5" style="font-weight: 700;">Keywords</h3>
              <div class="content">
                <p>Benchmark, Gesture Interaction, Vision Models, Decision Support Tools</p>
              </div>
            </div>
          </div>

          <h2 class="title is-4" style="font-weight: 700;" id="citation">Citation</h2>
          <div class="content">
            <p>
              Rayan El Idrissi Dafali and Alina Glushkova. 2025. VIBE: An Open-Source Platform for Benchmarking Vision Models for Gesture-Based Interaction. In IHM ’25 Extended Abstracts of the 36th Francophone Conference on Human–Computer Interaction, November 25–28, 2025, Toulouse, France.
            </p>
          </div>

          <h2 class="title is-4" style="font-weight: 700;" id="contact">Authors • Contact</h2>
          <div class="content">
            <p>
              Rayan El Idrissi Dafali, Mines Paris – PSL, Centre de Robotique, Paris, France, <a href="mailto:rayan.el_idrissi_dafali@minesparis.psl.eu">rayan.el_idrissi_dafali@minesparis.psl.eu</a><br>
              Alina Glushkova, Mines Paris – PSL, Centre de Robotique, Paris, France, <a href="mailto:alina.glushkova@minesparis.psl.eu">alina.glushkova@minesparis.psl.eu</a>
            </p>
          </div>

          <h2 class="title is-4" style="font-weight: 700;" id="introduction">1. Introduction</h2>
          <div class="content has-text-justified">
            <p>
              Gesture input offers a natural interaction channel but remains challenging to engineer: models can be accurate yet hard to integrate with real-time constraints and heterogeneous hardware. Comparing accuracy, latency, and resource usage across models is often tedious and unreproducible.
            </p>
            <p>
              <b>VIBE</b> addresses this by providing a unified, zero-setup web platform. Users upload a short video, select one or more pretrained models, and immediately obtain key metrics and visualizations to guide model selection for interactive HCI scenarios.
            </p>
          </div>

          <h2 class="title is-4" style="font-weight: 700;" id="system">2. System Design and Implementation</h2>
          <div class="content has-text-justified">
            <p>
              VIBE is a modular, interactive benchmarking environment for gesture-oriented vision models. It supports no-code evaluation while recording per-frame performance signals.
            </p>
            <p>
              The UI provides drag‑and‑drop video upload (≤ 30s). After upload, users pick models and metrics; results appear in a dashboard. Each model runs inside a versioned Docker container (CPU/GPU), ensuring stability and reproducibility with automatic resource-aware scheduling.
            </p>
            <p>
              Up to ~20 metrics are reported across four categories: accuracy/quality, performance/latency, resource usage, and robustness. Latency is contextualized via calibrated profiles (mobile GPU, embedded CPUs, workstations) to match deployment scenarios.
            </p>
          </div>

          <h2 class="title is-4" style="font-weight: 700;" id="architecture">Architecture</h2>
          <div class="content has-text-centered">
            <img src="./resources/architecture.png" alt="VIBE Architecture" style="width: 90%; margin: 0 auto; display: block;">
          </div>

          <h3 class="title is-5" style="font-weight: 700;">Table 1 – Local execution vs VIBE (web app)</h3>
          <div class="table-container">
            <table class="table is-striped is-hoverable is-fullwidth">
              <thead>
                <tr>
                  <th>Criterion</th>
                  <th>Local execution (workstation)</th>
                  <th>VIBE (web app)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Setup & Accessibility</td>
                  <td>Manual installs and configuration; technical expertise required</td>
                  <td>Zero setup; runs in the browser</td>
                </tr>
                <tr>
                  <td>Reproducibility & Maintenance</td>
                  <td>Fragile to local updates and environment drift</td>
                  <td>Versioned Docker containers; pinned, reproducible environment</td>
                </tr>
                <tr>
                  <td>Performance & Resources</td>
                  <td>Direct control of local CPU/GPU but limited by hardware</td>
                  <td>Dynamic CPU/GPU allocation</td>
                </tr>
                <tr>
                  <td>Model Updates</td>
                  <td>Manual, non-automated updates</td>
                  <td>Continuous deployment via CI/CD with versioning</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h2 class="title is-4" style="font-weight: 700;" id="discussion">3. Discussion and Future Work</h2>
          <div class="content has-text-justified">
            <p>
              VIBE streamlines model selection for interactive systems by combining rapid prototyping with rigorous benchmarking. In minutes, researchers can evaluate models, compare them side‑by‑side, and export results for decision support. Future work includes broader task coverage, privacy controls, and expanded robustness testing.
            </p>
          </div>

          <h2 class="title is-4" style="font-weight: 700;" id="acknowledgements">Acknowledgements</h2>
          <div class="content has-text-justified">
            <p>
              This work was supported by the French Government under the “Compétences et Métiers d’Avenir” project “ReSource” within the France 2030 program, led by the Caisse des Dépôts.
            </p>
          </div>

          <h2 class="title is-4" style="font-weight: 700;" id="references">References</h2>
          <div class="content">
            <ol>
              <li>Bazarevsky, V. et al. 2020. BlazePose: On-device Real-time Body Pose Tracking. CVPRW.</li>
              <li>Cao, Z. et al. 2017. Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. CVPR.</li>
              <li>Jiang, T. et al. 2023. RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose. arXiv:2303.07399.</li>
              <li>Lin, F. et al. 2021. Two-Hand Global 3D Pose Estimation Using Monocular RGB. WACV.</li>
              <li>Lu, P. et al. 2023. RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose Estimation. arXiv:2312.07526.</li>
              <li>Moon, G. et al. 2022. Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation. CVPRW.</li>
              <li>Yang, Z. et al. 2023. Effective Whole-body Pose Estimation with Two-stages Distillation. arXiv:2307.15880.</li>
              <li>Yu, C. et al. 2021. Lite-HRNet: A Lightweight High-Resolution Network. CVPR.</li>
              <li>Zhang, F. et al. 2020. MediaPipe Hands: On-device Real-time Hand Tracking. CVPRW.</li>
            </ol>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-4" style="font-weight: 700;">BibTeX</h2>
      <pre><code>@inproceedings{elidrissidafali2025vibe,
  title={VIBE: An Open-Source Platform for Benchmarking Vision Models for Gesture-Based Interaction},
  author={El Idrissi Dafali, Rayan and Glushkova, Alina},
  booktitle={IHM '25 Extended Abstracts of the 36th Francophone Conference on Human--Computer Interaction},
  year={2025},
  address={Toulouse, France}
}</code></pre>
    </div>
  </section>

  

  <section class="section" id="Acknowledgements" style="padding-top: 0rem;">
    <div class="container is-max-desktop content">
      <h2 class="title is-4" style="font-weight: 700;">Legal</h2>
      <p>This page is a research demo site. Content © 2025 R. El Idrissi Dafali, A. Glushkova.</p>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This webpage template is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
  

</body>

</html>
